# 进度

## 关于训练慢

其实只有highway这个子任务比较慢，十字路口和环岛都挺快的，环岛一分钟能跑4000个step。然后你问问助教能不能修改reward函数，把车限定在马路上，就是在训练的时候修改，eval的时候还是用原来的。感觉这样会快很多。

## 封装

简单的封装已经好了，但是我感觉这个原本的文件就有些问题，后面可能还要参照你的那份改一改。

- main.py 主模块
- makeEnv.py 建造环境
- ReplayBuffer.py 缓存
- SAC.py SAC类和两个网络，store和load我已经加进去了，我还没试过能不能用
- evalApi.py 和eval的交互，一样我还没试过能不能用，那个不同目录下的文件调用好像要写__init__.py，你查一下

## 接下来要做的

### 继续封装

- parse_args，加可以调整的超参
- 网络优化，感觉初始的那个网络怪怪的，可以按你的调一下
- 打印信息和存储tensorboard
- 对gpu的支持，初始版本是不支持gpu的，我看网上说gpu和cpu跑得差不多快是因为没有优化，可以部分任务使用cpu部分使用gpu，这样会比较快。但我感觉这个其实无关紧要，可以留到后面。

### 改reward

你可以去问问助教能不能改reward，可以的话看看怎么改，重写类的话上网搜应该能搜到，还有一种思路是可以看"on_road"这个状态，判断车在不在路上，不在的话直接给一个大的惩罚。从而push车一直在路上。但是这样的话泛化性可能就不够了。

### 开始优化

还有的话我觉得基本可以跑起来（环路）的话可以开始优化了，直接先做priority ReplayBuffer，这个你去找冯奕哲，看他能不能接受，我是觉得再搞一个PPO没有太大的意义。

  ----re: 
  1. em我确实也觉得意义不是很大，但这样的话在做priority replayBuffer之前是不是就不能调参🤔，还是说我们先调一版，如果冯奕哲同意做replayBuffer的话，等他做好了我们接着再调一下
  2. 网络结构的话我看过了，基本上是一样的，除了有一个地方有点区别，见SAC.py line 60，那里直接把第一个网络得到的s_a又通过第二个网络了，我不知道这这样妥不妥，在cleanRL里面，两个网络是完全独立的，应写做q2 = torch.cat([s, a], 1)，等你醒了讨论一下
  3. 另外，SAC.py 中的 actor 的 forward 函数，相当于把 cleanRL 中的forward 和 get_action 揉进一个函数里面了，但我觉得不需要改，因为 cleanRL 的主函数中本身就只用到了actor.get_action()，而 get_action 又调用 actor(x)，所以只是写法不一样，结构是一样的(note : 在继承了nn.module 以后，调用actor(x) 就相当于 actor.forward(x)，DQN里面遇到过。)
  4. 输入输出和args我之后加上
     
